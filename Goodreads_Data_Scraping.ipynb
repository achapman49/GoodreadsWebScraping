{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPF27rgjy7onPpNgFfUVLiJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achapman49/GoodreadsWebScraping/blob/main/Goodreads_Data_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dependencies"
      ],
      "metadata": {
        "id": "OzWqErj94UGy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TXzaQUWk3h-5",
        "outputId": "0307ec5e-9cd7-4a2a-cd4b-4b3623fab76e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.8/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from bs4) (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: progressbar\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12081 sha256=ce08891301d54052c11587da09c5e4421ffed8ac4d58fd675ebce7909e29d892\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/67/ed/d84123843c937d7e7f5ba88a270d11036473144143355e2747\n",
            "Successfully built progressbar\n",
            "Installing collected packages: progressbar\n",
            "Successfully installed progressbar-2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "progressbar"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gender_guesser\n",
            "  Downloading gender_guesser-0.4.0-py2.py3-none-any.whl (379 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.3/379.3 KB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gender_guesser\n",
            "Successfully installed gender_guesser-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting translators\n",
            "  Downloading translators-5.5.6-py3-none-any.whl (33 kB)\n",
            "Collecting pathos>=0.2.9\n",
            "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cryptography>=38.0.1\n",
            "  Downloading cryptography-39.0.0-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests>=2.28.1\n",
            "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.8/dist-packages (from translators) (4.9.2)\n",
            "Collecting PyExecJS>=1.5.1\n",
            "  Downloading PyExecJS-1.5.1.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=38.0.1->translators) (1.15.1)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from pathos>=0.2.9->translators) (0.3.6)\n",
            "Collecting pox>=0.3.2\n",
            "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
            "Collecting ppft>=1.7.6.6\n",
            "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess>=0.70.14\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from PyExecJS>=1.5.1->translators) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28.1->translators) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28.1->translators) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28.1->translators) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.28.1->translators) (2.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=38.0.1->translators) (2.21)\n",
            "Building wheels for collected packages: PyExecJS\n",
            "  Building wheel for PyExecJS (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-py3-none-any.whl size=14596 sha256=46cf234f17c20583277ed57a581bff94465de354c25687d065e4d87abb89b28e\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/3c/3d/7e9aca234caf6602ae4a4c7b367b3afc03519e791b998a94e4\n",
            "Successfully built PyExecJS\n",
            "Installing collected packages: requests, PyExecJS, ppft, pox, multiprocess, pathos, cryptography, translators\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.25.1\n",
            "    Uninstalling requests-2.25.1:\n",
            "      Successfully uninstalled requests-2.25.1\n",
            "Successfully installed PyExecJS-1.5.1 cryptography-39.0.0 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 requests-2.28.2 translators-5.5.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from langdetect) (1.15.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=834fdfdca4a5c19900e1249f35f6c4cf22550b9fb80c07812d2423705bd9b7a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4\n",
        "!pip install progressbar\n",
        "!pip install gender_guesser\n",
        "!pip install translators --upgrade\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Imports"
      ],
      "metadata": {
        "id": "E-82qcfj4Ywb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Standard imports for scraping and manipulating data\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "\n",
        "#Progressbar is helpful to see the progress of functions which take a long time to run\n",
        "import progressbar\n",
        "\n",
        "from datetime import datetime\n",
        "import ast\n",
        "\n",
        "#Gender guesser is used to try and parse author gender\n",
        "import gender_guesser.detector as gender\n",
        "\n",
        "#Translator for non-English-language records and language detector to find them\n",
        "import translators as ts\n",
        "import translators.server as tss\n",
        "from langdetect import detect\n",
        "from langdetect import DetectorFactory\n",
        "DetectorFactory.seed = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQZ4ryVi4eDe",
        "outputId": "9e82bc57-2867-4cfa-822e-214e26d9d8d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using state District of Columbia server backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Scrape a list"
      ],
      "metadata": {
        "id": "50rMUPpU441R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Define Function"
      ],
      "metadata": {
        "id": "OWJwm-Z05FwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this to define the function for scraping the list"
      ],
      "metadata": {
        "id": "SZ1nz4Rj4-yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the function for scraping every page of a list from a url\n",
        "\n",
        "def scrape_book_list(url, show_df = False, show_progress = False):\n",
        "\n",
        "    ##Define urls and create the initial html soup\n",
        "    if show_progress == True:\n",
        "        print('Turning page into soup...')\n",
        "    goodreads_url = \"https://www.goodreads.com\"\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    \n",
        "    #Get list length\n",
        "    if show_progress == True:\n",
        "        print('Getting list length...')\n",
        "    \n",
        "    pagination = soup.find(\"div\", class_ = \"pagination\").find_all(\"a\")\n",
        "    while pagination == None:\n",
        "        time.sleep(3)\n",
        "        pagination = soup.find(\"div\", class_ = \"pagination\").find_all(\"a\")\n",
        "\n",
        "\n",
        "    ##For navigation, I need to find out how many pages there are in this list\n",
        "    pages = []\n",
        "    for a in pagination:\n",
        "        pages.append(a.text)\n",
        "    max_page = int(pages[-2]) #The last item is always the word ' next', take the next to last\n",
        "    \n",
        "    ##Now create a list of links\n",
        "    ### The pages in these lists are defined by adding '?page=[i]' at the end, making them easy to iterate over\n",
        "    list_page_links = []\n",
        "    for i in range(2, max_page + 1):\n",
        "        list_page_link = f\"{link}?page={i}\"\n",
        "        list_page_links.append(list_page_link)\n",
        "    \n",
        "    ##The lists are in a table\n",
        "    if show_progress == True:\n",
        "        print('Scraping list table on page 1...')\n",
        "    list_table = soup.find(\"table\", class_= \"tableList\")\n",
        "    \n",
        "    ##Get the relevant page elements to populate the columns with content from the first page\n",
        "    rankings = [row.find(\"td\").text.strip() for row in list_table.find_all(\"tr\")]\n",
        "    titles = [row.find(\"a\", class_=\"bookTitle\").text.strip() for row in list_table.find_all(\"tr\")]\n",
        "    authors = [row.find(\"a\", class_=\"authorName\").text.strip() for row in list_table.find_all(\"tr\")]\n",
        "    links = [row[\"href\"] for row in list_table.find_all(\"a\", class_=\"bookTitle\")]\n",
        "    \n",
        "    ##Scores and votes are stored in such a way that I need to use a different method\n",
        "    table_rows = soup.find_all(\"tr\")\n",
        "\n",
        "    scores = []\n",
        "    votes = []\n",
        "\n",
        "    for row in table_rows:\n",
        "        extra_info = [element.text for element in row.find_all(\"a\", href = \"#\")]\n",
        "        score = int(extra_info[0].replace(\"score: \",\"\").replace(\",\", \"\"))\n",
        "        vote_count = int(extra_info[1].replace(\" people voted\", \"\").replace(\" person voted\", \"\").replace(\",\", \"\"))\n",
        "        scores.append(score)\n",
        "        votes.append(vote_count)    \n",
        "        \n",
        "    ##Ratings and counts of ratings\n",
        "    ratings_and_counts = soup.find_all(\"span\", class_ = \"minirating\")\n",
        "\n",
        "    avg_ratings  = []\n",
        "    rating_counts = []\n",
        "\n",
        "    for i in ratings_and_counts:\n",
        "        text = i.text\n",
        "        text = text.replace(\"it was amazing \", \"\").replace(\"really liked it \", \"\").replace(\"liked it \", \"\").replace(\"it was ok \", \"\").replace(\"did not like it \", \"\")\n",
        "        text = text.replace(\" avg rating \", \"\").replace(\" ratings\", \"\").replace(\" rating\",\"\").replace(\",\", \"\")\n",
        "        text = text.split(\"— \")\n",
        "        avg_ratings.append(float(text[0]))\n",
        "        rating_counts.append(int(text[1]))\n",
        "\n",
        "    \n",
        "    ##Loop through and append content from the other pages\n",
        "    if show_progress == True:\n",
        "        print('Scraping additional list pages...')\n",
        "        \n",
        "    bar = progressbar.ProgressBar(maxval=max_page).start()\n",
        "    \n",
        "    for l in list_page_links:\n",
        "        page = requests.get(l)\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        list_table = soup.find(\"table\", class_= \"tableList\")\n",
        "        while list_table == None:\n",
        "            time.sleep(3)\n",
        "            page = requests.get(l)\n",
        "            soup = BeautifulSoup(page.content, 'html.parser')\n",
        "            list_table = soup.find(\"table\", class_= \"tableList\")\n",
        "        \n",
        "        rankings_new = [row.find(\"td\").text.strip() for row in list_table.find_all(\"tr\")]\n",
        "        titles_new = [row.find(\"a\", class_=\"bookTitle\").text.strip() for row in list_table.find_all(\"tr\")]\n",
        "        authors_new = [row.find(\"a\", class_=\"authorName\").text.strip() for row in list_table.find_all(\"tr\")]\n",
        "        links_new = [row[\"href\"] for row in list_table.find_all(\"a\", class_=\"bookTitle\")]\n",
        "        \n",
        "        ###Scores and votes\n",
        "        table_rows = soup.find_all(\"tr\")\n",
        "\n",
        "        scores_new = []\n",
        "        votes_new= []\n",
        "\n",
        "        for row in table_rows:\n",
        "            extra_info = [element.text for element in row.find_all(\"a\", href = \"#\")]\n",
        "            score = int(extra_info[0].replace(\"score: \",\"\").replace(\",\", \"\"))\n",
        "            vote_count = int(extra_info[1].replace(\" people voted\", \"\").replace(\" person voted\", \"\").replace(\",\", \"\"))\n",
        "            scores_new.append(score)\n",
        "            votes_new.append(vote_count)  \n",
        "        \n",
        "        #Ratings and counts\n",
        "        ratings_and_counts_new = soup.find_all(\"span\", class_ = \"minirating\")\n",
        "\n",
        "        avg_ratings_new  = []\n",
        "        rating_counts_new = []\n",
        "\n",
        "        for i in ratings_and_counts_new:\n",
        "            text = i.text\n",
        "            text = text.replace(\"it was amazing \", \"\").replace(\"really liked it \", \"\").replace(\"liked it \", \"\").replace(\"it was ok \", \"\").replace(\"did not like it \", \"\")\n",
        "            text = text.replace(\" avg rating \", \"\").replace(\" ratings\", \"\").replace(\" rating\",\"\").replace(\",\", \"\")\n",
        "            text = text.split(\"— \")\n",
        "            avg_ratings_new.append(float(text[0]))\n",
        "            rating_counts_new.append(int(text[1]))\n",
        "\n",
        "        rankings.extend(rankings_new)\n",
        "        titles.extend(titles_new)\n",
        "        authors.extend(authors_new)\n",
        "        links.extend(links_new)\n",
        "        scores.extend(scores_new)\n",
        "        votes.extend(votes_new)\n",
        "        avg_ratings.extend(avg_ratings_new)\n",
        "        rating_counts.extend(rating_counts_new)\n",
        "        \n",
        "        bar.update(+1)\n",
        "        \n",
        "    bar.finish()\n",
        "    \n",
        "    #Format links correctly\n",
        "    links = [f\"{goodreads_url}{row}\" for row in links]\n",
        "        \n",
        "    ##Create a dataframe to easily view and manipulate the data\n",
        "    global list_df\n",
        "    list_df = pd.DataFrame(columns=[\"Rank\", \"Title\", \"Author\", \"URL\", \"Score\", \"Votes\"])\n",
        "    list_df[\"Rank\"] = rankings\n",
        "    list_df[\"Title\"] = titles\n",
        "    list_df[\"Author\"] = authors\n",
        "    list_df[\"URL\"] = links\n",
        "    list_df[\"Score\"] = scores\n",
        "    list_df[\"Votes\"] = votes\n",
        "    list_df[\"Average Rating\"] = avg_ratings\n",
        "    list_df[\"Rating Count\"] = rating_counts\n",
        "    \n",
        "\n",
        "        \n",
        "    ##Option to show dataframe at the end\n",
        "    if show_df == True:\n",
        "        list_df.head()"
      ],
      "metadata": {
        "id": "x06lIu5a480D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Define which List to Scrape"
      ],
      "metadata": {
        "id": "-jw5rcdQ5Mlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input the values you want here"
      ],
      "metadata": {
        "id": "478M83Xt857Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* It's very important that you remove the page tags from the end of link (the '?page=n' part). Otherwise this will not work.\n",
        "* If you tick the 'view_progress' box the functionis going to output every piece of data it scrapes so you can watch it scraping.\n",
        "* 'Up to page' let's you stop the scraping at a certain page - good if you are only interested in the first x pages. Leave blank to scrape all pages (WARNING: This can be time-consuming on long lists)"
      ],
      "metadata": {
        "id": "BuPd9TId7oaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_link = \"https://www.goodreads.com/list/show/1.Best_Books_Ever\" #@param {type:\"string\"}\n",
        "view_progess = False #@param {type:\"boolean\"}\n",
        "up_to_page = \"\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "LcRqK3TI5Lai"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VgbSGxdV8oQt",
        "outputId": "189fe1f1-fc70-4f07-bde7-4e7908f2fd36"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}